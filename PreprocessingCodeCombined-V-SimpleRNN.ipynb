{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random as rn\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "# Use a fixed seed for the random number generator to address randomness problem and get reproducable results with keras. the numbers don't make much difference.\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158784it [12:46, 207.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sentences=[]\n",
    "words = []\n",
    "targets=[] \n",
    "sen_position=0 \n",
    "\n",
    "labels= {'Correct':0, 'ArtOrDet':1, 'Nn':2, 'Prep':3, 'SVA':4, 'Vform':5, 'Vt':6, 'Wform':7}\n",
    "\n",
    " \n",
    "# Import the sentences from the data file\n",
    "f=open('/Users/highsierra/Tech-Skills/Unorganised/conll2014-master/release3.2/data/conll14st-preprocessed.m2')\n",
    "for line in tqdm(f):\n",
    "    parts = line.split()\n",
    "    if(len(parts)>0):\n",
    "        if line[0]=='S':\n",
    "            # Cleaning\n",
    "            line = line[2:].strip()\n",
    "            \n",
    "            # List of sentences\n",
    "            sentences.append(line)\n",
    "\n",
    "            # List of words, for the vocabulary info\n",
    "            words = words + line.split()\n",
    "\n",
    "            \n",
    "            # Initial Outputs Processing  \n",
    "            \n",
    "            # By default, consider every word as non-erroneous, by creating an array with the tag \"Correct\" per every word.\n",
    "            tags=np.zeros(shape=(len(parts)-1), dtype='int32')\n",
    "            # Combine the tags associated with each sentence vertically in order to allign them with the input words\n",
    "            targets.append(tags)\n",
    "            # Keep track of the sentence's position\n",
    "            sen_position += 1\n",
    "            \n",
    "        elif parts[0]=='A':\n",
    "            if re.findall(\"ArtOrDet\", parts[2]) or re.findall(\"Nn\", parts[2]) or re.findall(\"Vt\", parts[2]) or re.findall(\"Prep\", parts[2]) or re.findall(\"Vform\", parts[2]) or re.findall(\"Wform\", parts[2]) or re.findall(\"SVA\", parts[2]):\n",
    "                # Keep track of the erroneous word's position by extracting it from the sentence annotation \n",
    "                digit = [int(j) for j in re.findall(\"[0-9]+\", parts[2][:2])]            \n",
    "    \n",
    "                for key in labels:\n",
    "                    if  re.search(key, parts[2]):\n",
    "                        err = labels.get(key)\n",
    "                 \n",
    "                # Using its extracted position, place the erroneous word's tag in its sentence\n",
    "                targets[sen_position - 1][digit[0]-1] = err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "sen_len = []\n",
    "for sen in sentences:\n",
    "    sen_len.append(len(sen.split()))\n",
    "    \n",
    "MAX_SEQUENCE_LENGTH = max(sen_len)\n",
    "print(MAX_SEQUENCE_LENGTH) #print(leng.index(max(sen_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33762\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = len(set(words))\n",
    "print(MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pad_sequences(targets, maxlen=MAX_SEQUENCE_LENGTH, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57151, 222)\n",
      "(57151, 222)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition sentences it into training and test sets: 80%, 20%\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout,Reshape, SimpleRNN, Bidirectional\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_(x_tr, y_tr, epos=3, my_batch_size=32):  \n",
    "    input = Input(shape=(MAX_SEQUENCE_LENGTH,)) # This returns a tensor. The comma is necessary when you have only one dimension.\n",
    "    model = Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=50, input_length=MAX_SEQUENCE_LENGTH, name=\"Embedding\")(input) # output_dim if 50 then the nw will learn 50-dimentional embeddings for each word. # This embedding layer will encode the input sequence into a sequence of dense 50-dimensional vectors.\n",
    "    model = SimpleRNN(units=50, return_sequences=True, recurrent_dropout=0.2)(model)\n",
    "    model = Dropout(0)(model)\n",
    "    out = TimeDistributed(Dense(8, activation='softmax'))(model)\n",
    "    model = Model(input, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_(X_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training and testing the network we also need to change the labels y to categorial.\n",
    "\n",
    "ycat_train = to_categorical(y_train, num_classes=8)\n",
    "ycat_test = to_categorical(y_test, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      "45720/45720 [==============================] - 388s 8ms/step - loss: 0.0406 - accuracy: 0.9939\n",
      "Epoch 2/3\n",
      "45720/45720 [==============================] - 396s 9ms/step - loss: 0.0127 - accuracy: 0.9983\n",
      "Epoch 3/3\n",
      "45720/45720 [==============================] - 554s 12ms/step - loss: 0.0102 - accuracy: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a7cd17950>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, ycat_train, epochs=3, batch_size=32, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Calculate the following metrics:\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11431/11431 [==============================] - 26s 2ms/step\n",
      "Accuracy: 99.83%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, ycat_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11431/11431 [==============================] - 21s 2ms/step\n",
      "(11431, 222, 8)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test, verbose=1) \n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(pred,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863285815158046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "pres_score = []\n",
    "\n",
    "for tru,pred in zip (y_test, y_pred):\n",
    "    pres_score.append(precision_score(tru,pred,average='macro'))\n",
    "\n",
    "precision = np.mean(pres_score)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8639421058768345\n"
     ]
    }
   ],
   "source": [
    "rec_score = []\n",
    "\n",
    "for tru,pred in zip (y_test, y_pred):\n",
    "    rec_score.append(recall_score(tru,pred,average='macro'))\n",
    "\n",
    "recall= np.mean(rec_score)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8636089662290262\n"
     ]
    }
   ],
   "source": [
    "# Manually, it's calculated according to the formula:  f1 = (2 * precision * recall) / (precision + recall)\n",
    "f_score = []\n",
    "\n",
    "for tru,pred in zip (y_test, y_pred):\n",
    "    f_score.append(f1_score(tru,pred,average='macro'))\n",
    "f1 = np.mean(f_score)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the experiments or runs using a SimpleRNN architecture:\n",
    "\n",
    "Experiments for: Finding the optimal Dropout value from the list [0, 0.1, 0.2]\n",
    "#### Run 1:  (a.k.a this one)\n",
    "###### The Variable: \n",
    "    Dropout = 0 \n",
    "    \n",
    "###### Defaults:\n",
    "    Number of layers = 1\n",
    "    Number of units or neurons within a layer = 50\n",
    "    Embedding size = 50\n",
    "    Batch size = 32\n",
    "    \n",
    "###### Constants:\n",
    "       Recurrent Dropout = 0.2\n",
    "       \n",
    "#### Run 2:  (a.k.a the next execution)\n",
    "###### The Variable: \n",
    "    Dropout = 0.1 \n",
    "    \n",
    "###### Defaults:\n",
    "    same as previous values\n",
    "    \n",
    "###### Constants:\n",
    "    same as previous values\n",
    "     \n",
    "#### Run 3:  (a.k.a the final execution, or the experiment using the last proposed value for Dropout)\n",
    "###### The Variable: \n",
    "    Dropout = 0.2 \n",
    "    \n",
    "###### Defaults:\n",
    "    same as previous values\n",
    "    \n",
    "###### Constants:\n",
    "    same as previous values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow1)",
   "language": "python",
   "name": "tensorflow1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
