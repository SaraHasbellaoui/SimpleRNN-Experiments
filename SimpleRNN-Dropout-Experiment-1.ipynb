{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random as rn\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "# Use a fixed seed for the random number generator to address randomness problem and get reproducable results with keras. the numbers don't make much difference.\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. Prepare a dataframe with: word, sentence #, tag\n",
    "2. Group words with their tags within their sentence\n",
    "3. Encode words and their tags. Then pad the sequences aka create a multi-label and multi-output classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158784it [11:27, 230.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57151\n",
      "1161567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the sentences from the data file\n",
    "f=open('/Users/highsierra/Tech-Skills/Unorganised/conll2014-master/release3.2/data/conll14st-preprocessed.m2')\n",
    "\n",
    "sentences = []\n",
    "words = []\n",
    "Stat = []\n",
    "\n",
    "\n",
    "word_tags=[] # the target output of each word \n",
    "error_tags = [\"ArtOrDet\",\"Nn\",\"Vt\",\"Prep\",\"Vform\",\"Wform\",\"SVA\"] # the seven grammatical errors' tags \n",
    "sen_position=0 \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "        \n",
    "for line in tqdm(f):\n",
    "    parts = line.split()\n",
    "    if(len(parts)>0):\n",
    "        if line[0]=='S':\n",
    "        # Initial Inputs Processing \n",
    "            # Perform basic cleaning\n",
    "            cleanSen = re.sub(r\"\\n\", '', line[2:])\n",
    "            \n",
    "            # Keep track of the sentences' lengths\n",
    "            Stat.append(len(cleanSen.split()))\n",
    "            \n",
    "            # Create a list of sentences\n",
    "            sentences.append(cleanSen)\n",
    "            \n",
    "            # Create a one-dimensional array of input words \n",
    "            words = words + cleanSen.split()\n",
    "            \n",
    "            \n",
    "        # Initial Outputs Processing  \n",
    "            \n",
    "            # By default, consider every word as non-erroneous, by creating an array with the tag \"Correct\" per every word.\n",
    "            tags=np.empty(shape=(len(parts)-1), dtype=object)\n",
    "            tags = np.where(tags==None, \"Correct\", tags)\n",
    "            # Combine the tags associated with each sentence vertically in order to allign them with the input words\n",
    "            word_tags.append(tags)\n",
    "            # Keep track of the sentence's position\n",
    "            sen_position += 1\n",
    "            \n",
    "        elif parts[0]=='A':\n",
    "            if re.findall(\"ArtOrDet\", parts[2]) or re.findall(\"Nn\", parts[2]) or re.findall(\"Vt\", parts[2]) or re.findall(\"Prep\", parts[2]) or re.findall(\"Vform\", parts[2]) or re.findall(\"Wform\", parts[2]) or re.findall(\"SVA\", parts[2]):\n",
    "                # Keep track of the erroneous word's position by extracting it from the sentence annotation \n",
    "                digit = [int(j) for j in re.findall(\"[0-9]+\", parts[2][:2])]            \n",
    "                  \n",
    "                # Extract the erroneous words tag\n",
    "                for tag in error_tags:\n",
    "                    if  re.search(tag, parts[2]):\n",
    "                        err = re.findall(tag, parts[2])\n",
    "                        \n",
    "                # Using its extracted position, place the erroneous word's tag in its sentence\n",
    "                word_tags[sen_position - 1][digit[0]-1] = err[0]\n",
    "            \n",
    "print(len(sentences))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tags to a one-dimensional array in order to allign with input words\n",
    "flattened_word_tags = []\n",
    "for sen in word_tags:\n",
    "    \n",
    "    for tag in sen:\n",
    "        flattened_word_tags.append(tag)\n",
    "#print(len(flattened_word_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label of the sentence's number for each word\n",
    "\n",
    "i = 1\n",
    "numbers = []\n",
    "for sen in sentences:\n",
    "    for word in range(len(sen.split())):\n",
    "        numbers.append(\"Sentence: \" + str(i))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161567\n",
      "1161567\n",
      "1161567\n"
     ]
    }
   ],
   "source": [
    "print(len(numbers))\n",
    "print(len(words))\n",
    "print(len(flattened_word_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>CREATING</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>A</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>HABITABLE</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>ENVIRONMENT</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Humans</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161562</th>\n",
       "      <td>Sentence: 57151</td>\n",
       "      <td>2009</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161563</th>\n",
       "      <td>Sentence: 57151</td>\n",
       "      <td>from</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161564</th>\n",
       "      <td>Sentence: 57151</td>\n",
       "      <td>http</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161565</th>\n",
       "      <td>Sentence: 57151</td>\n",
       "      <td>:</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161566</th>\n",
       "      <td>Sentence: 57151</td>\n",
       "      <td>//sg.news.yahoo.com/ap/20091001/twl-us-nyc-ter...</td>\n",
       "      <td>Correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1161567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #                                               Word  \\\n",
       "0            Sentence: 1                                           CREATING   \n",
       "1            Sentence: 1                                                  A   \n",
       "2            Sentence: 1                                          HABITABLE   \n",
       "3            Sentence: 1                                        ENVIRONMENT   \n",
       "4            Sentence: 2                                             Humans   \n",
       "...                  ...                                                ...   \n",
       "1161562  Sentence: 57151                                               2009   \n",
       "1161563  Sentence: 57151                                               from   \n",
       "1161564  Sentence: 57151                                               http   \n",
       "1161565  Sentence: 57151                                                  :   \n",
       "1161566  Sentence: 57151  //sg.news.yahoo.com/ap/20091001/twl-us-nyc-ter...   \n",
       "\n",
       "             Tag  \n",
       "0        Correct  \n",
       "1        Correct  \n",
       "2        Correct  \n",
       "3        Correct  \n",
       "4        Correct  \n",
       "...          ...  \n",
       "1161562  Correct  \n",
       "1161563  Correct  \n",
       "1161564  Correct  \n",
       "1161565  Correct  \n",
       "1161566  Correct  \n",
       "\n",
       "[1161567 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(list(zip(numbers, words, flattened_word_tags)), \n",
    "               columns =['Sentence #','Word', 'Tag']) \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33763\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# For counting only\n",
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words); \n",
    "print(n_words)\n",
    "\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags); \n",
    "print(n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SentenceGetter retrieves sentences with their labels.\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57151\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "sentences = getter.sentences\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "leng = []\n",
    "for sen in sentences:\n",
    "    leng.append(len(sen))\n",
    "print(max(leng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33763\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = list(set(data[\"Word\"].values))\n",
    "corpus_tokens.append(\"ENDPAD\")\n",
    "\n",
    "#print(corpus_tokens)\n",
    "print(len(corpus_tokens))\n",
    "\n",
    "Idx = range(0, len(corpus_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>//www.technologyreview.com/infotech/18537/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>//news.bbc.co.uk/2/hi/programmes/from_our_own_...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contrast</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33758</th>\n",
       "      <td>executing</td>\n",
       "      <td>33758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33759</th>\n",
       "      <td>Table</td>\n",
       "      <td>33759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33760</th>\n",
       "      <td>homeowners</td>\n",
       "      <td>33760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33761</th>\n",
       "      <td>out-break</td>\n",
       "      <td>33761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33762</th>\n",
       "      <td>ENDPAD</td>\n",
       "      <td>33762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33763 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Word     No\n",
       "0                                                    1.5      0\n",
       "1                                                 covers      1\n",
       "2             //www.technologyreview.com/infotech/18537/      2\n",
       "3      //news.bbc.co.uk/2/hi/programmes/from_our_own_...      3\n",
       "4                                               contrast      4\n",
       "...                                                  ...    ...\n",
       "33758                                          executing  33758\n",
       "33759                                              Table  33759\n",
       "33760                                         homeowners  33760\n",
       "33761                                          out-break  33761\n",
       "33762                                             ENDPAD  33762\n",
       "\n",
       "[33763 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = pd.DataFrame(list(zip(corpus_tokens, Idx)), \n",
    "               columns =['Word', 'No']) \n",
    "word2idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed=[(w, n) for w, n in zip(word2idx[\"Word\"].values.tolist(), word2idx[\"No\"].values.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57151/57151 [1:36:11<00:00,  9.90it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "XX=[]\n",
    "X=[]\n",
    "\n",
    "for s in tqdm(sentences):\n",
    "    XX=[]\n",
    "    for w in s:\n",
    "        for I in indexed:\n",
    "            if I[0] == w[0]:\n",
    "                XX.append(I[1])\n",
    "                break\n",
    "    X.append(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word     No\n",
      "33762  ENDPAD  33762\n"
     ]
    }
   ],
   "source": [
    "print(word2idx.loc[word2idx['Word'] == \"ENDPAD\"])\n",
    "#We pad the sentences with the index value of \"ENDPAD\", which is: 33762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len = max(leng) + 1 # or 223\n",
    "\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=33762)#pad with last word of index \n",
    "#n_words-1 is the index of “ENDPAD” in word2idx. Cleaner would be to use word2idx[“ENDPAD”] as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57151/57151 [00:01<00:00, 45068.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels= {'ArtOrDet':0, 'Correct':1, 'Nn':2, 'Prep':3, 'SVA':4, 'Vform':5, 'Vt':6, 'Wform':7}\n",
    "\n",
    "yy=[]\n",
    "y=[]\n",
    "\n",
    "for s in tqdm(sentences):\n",
    "    yy=[]\n",
    "    #print(s)\n",
    "    for w in s:\n",
    "        #print(w)\n",
    "        for key in labels:\n",
    "            if w[1] == key:\n",
    "                yy.append(labels.get(key))\n",
    "    y.append(yy)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57151, 223)\n",
      "(57151, 223)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition sentences it into training and test sets: 80%, 20%\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout,Reshape, SimpleRNN, Bidirectional\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the experiments or runs using a SimpleRNN architecture:\n",
    "\n",
    "Experiments for: Finding the optimal Dropout value from the list [0, 0.1, 0.2]\n",
    "#### Run 1:  (a.k.a this one)\n",
    "###### The Variable: \n",
    "    Dropout = 0 \n",
    "    \n",
    "###### Defaults:\n",
    "    Number of layers = 1\n",
    "    Number of units or neurons within a layer = 50\n",
    "    Embedding size = 50\n",
    "    Batch size = 32\n",
    "    \n",
    "###### Constants:\n",
    "       Recurrent Dropout = 0.2\n",
    "       \n",
    "#### Run 2:  (a.k.a the next execution)\n",
    "###### The Variable: \n",
    "    Dropout = 0.1 \n",
    "    \n",
    "###### Defaults:\n",
    "    same as previous values\n",
    "    \n",
    "###### Constants:\n",
    "    same as previous values\n",
    "     \n",
    "#### Run 3:  (a.k.a the final execution, or the experiment using the last proposed value for Dropout)\n",
    "###### The Variable: \n",
    "    Dropout = 0.2 \n",
    "    \n",
    "###### Defaults:\n",
    "    same as previous values\n",
    "    \n",
    "###### Constants:\n",
    "    same as previous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "def Model_(x_tr, y_tr, epos=3, my_batch_size=32):  \n",
    "    input = Input(shape=(max_len,)) # This returns a tensor. The comma is necessary when you have only one dimension.\n",
    "    model = Dropout(0)(model)\n",
    "    model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len, name=\"Embedding\")(input) # output_dim if 50 then the nw will learn 50-dimentional embeddings for each word. # This embedding layer will encode the input sequence into a sequence of dense 50-dimensional vectors.\n",
    "    model = SimpleRNN(units=50, return_sequences=True, recurrent_dropout=0.2)(model)\n",
    "    out = TimeDistributed(Dense(n_tags, activation='softmax'))(model)\n",
    "    model = Model(input, out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = Model_(X_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training and testing the network we also need to change the labels y to categorial.\n",
    "\n",
    "ycat_train = to_categorical(y_train, num_classes=n_tags)\n",
    "ycat_test = to_categorical(y_test, num_classes=n_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Calculate the following metrics:\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      "45720/45720 [==============================] - 304s 7ms/step - loss: 0.0450 - accuracy: 0.9929\n",
      "Epoch 2/3\n",
      "45720/45720 [==============================] - 293s 6ms/step - loss: 0.0123 - accuracy: 0.9983\n",
      "Epoch 3/3\n",
      "45720/45720 [==============================] - 286s 6ms/step - loss: 0.0101 - accuracy: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x64f5fd290>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, ycat_train, epochs=3, batch_size=32, verbose=1) \n",
    "#epoch costs 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11431/11431 [==============================] - 19s 2ms/step\n",
      "Accuracy: 99.84%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, ycat_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11431/11431 [==============================] - 16s 1ms/step\n",
      "(11431, 223, 8)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test, verbose=1) \n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(pred,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8658826787051885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "pres_score = []\n",
    "\n",
    "for tru,pred in zip (y_test, y_pred):\n",
    "    pres_score.append(precision_score(tru,pred,average='macro'))\n",
    "\n",
    "precision = np.mean(pres_score)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8664763785993718\n"
     ]
    }
   ],
   "source": [
    "rec_score = []\n",
    "\n",
    "for tru,pred in zip (y_test, y_pred):\n",
    "    rec_score.append(recall_score(tru,pred,average='macro'))\n",
    "\n",
    "recall= np.mean(rec_score)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8661666644676528\n"
     ]
    }
   ],
   "source": [
    "# Manually, it's calculated according to the formula:  f1 = (2 * precision * recall) / (precision + recall)\n",
    "f_score = []\n",
    "\n",
    "for tru,pred in zip (y_test, y_pred):\n",
    "    f_score.append(f1_score(tru,pred,average='macro'))\n",
    "f1 = np.mean(f_score)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow1)",
   "language": "python",
   "name": "tensorflow1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
